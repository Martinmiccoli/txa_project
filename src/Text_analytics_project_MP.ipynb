{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb5a9d-17f6-4f1d-b253-b2ac77192769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import spacy\n",
    "import stanza\n",
    "import gensim\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from collections import defaultdict\n",
    "from gensim import corpora\n",
    "import string\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be15d172-7e12-4b5c-8e34-a86c973771fb",
   "metadata": {},
   "source": [
    "Perplexity measures how much surprised is a model to observe some text\n",
    "- Given a test text W its (normalized) perplexity is:\n",
    "- High perplexity means that the text has got a low probability\n",
    "- Low perplexity means that the text has got a high probability\n",
    "\n",
    "• which is what we aim for, given that test texts are from the same source of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b61a4fb-9c7f-4126-8697-7cfcc9d59fa3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52a6962-9580-47c3-9424-ea95c97c9f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "training_sets_dir = \"haspeede2_dev\"\n",
    "training_file = \"haspeede2_dev_taskAB.tsv\"\n",
    "\n",
    "train_path = os.path.join(data_dir, training_sets_dir, training_file)\n",
    "\n",
    "## Columns resetting\n",
    "df = pd.read_table('haspeede2_dev_taskAB.tsv', header= 0)\n",
    "df = df.drop('stereotype', axis = 1)\n",
    "# Rename the column 'text ', there was an extraspace\n",
    "df.rename(columns = {'text ': 'text'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594ed710-c00d-4ae9-b4d5-3dbbaa4683a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    # Lowercase tweets\n",
    "    df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "\n",
    "    # Preprocess text in a single step\n",
    "    df['text_processed'] = df['text'].apply(lambda x: re.sub(r'@\\w+', '', x))  # Remove mentions\n",
    "    df['text_processed'] = df['text_processed'].apply(lambda x: re.sub(r'\\.{2,}', ' ', x))  # Remove multiple dots\n",
    "    df['text_processed'] = df['text_processed'].apply(lambda x: re.sub(r'\\d+', '', x))  # Remove numbers\n",
    "    df['text_processed'] = df['text_processed'].apply(lambda x: re.sub(r'#', '', x))  # Remove hashtags but keep text\n",
    "    df['text_processed'] = df['text_processed'].apply(lambda x: re.sub(r'url', '', x))  # Remove URLs\n",
    "    df['text_processed'] = df['text_processed'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())  # Remove extra spaces\n",
    "\n",
    "    custom_punctuation = string.punctuation + \"’‘\" + '’' + '' + '``' + \"''\"\n",
    "\n",
    "\n",
    "    # Tokenize into words\n",
    "    df['word_token'] = df['text_processed'].apply(nltk.word_tokenize)\n",
    "\n",
    "    # Remove stopwords and punctuation\n",
    "    df['tokenized'] = [[word for word in set(tweet)\n",
    "         if word in word not in stopwords.words('italian') and word not in custom_punctuation]  # Remove punctuation and stopwords\n",
    "        for tweet in df['word_token']]\n",
    "\n",
    "    # Tokenize into sentences\n",
    "    df['sentence_token'] = df['text_processed'].apply(nltk.sent_tokenize)\n",
    "    # Drop intermediate columns\n",
    "    df = df.drop('word_token', axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "e2b4d1ee-0570-4663-939d-a51b2ed5b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_df(df)\n",
    "# n° of tokens per tweet\n",
    "df['n_token'] = df['tokenized'].apply(len)\n",
    "# n° of sentences per tweet\n",
    "df['n_sentence'] = df['sentence_token'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53766ae-b459-45d6-838d-cf17374fd158",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Badwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc16316b-d4fc-4780-b1a1-19237a6e169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('lista_badwords.txt')\n",
    "badwords = file.readlines()\n",
    "badwords = [word.replace('\\n', '') for word in badwords]\n",
    "\n",
    "# How many badword per tweet?\n",
    "badword_0_1 = []\n",
    "for l in df['tokenized']:\n",
    "    counter = 0\n",
    "    for word in l:\n",
    "        if word in badwords:\n",
    "            counter += 1\n",
    "    badword_0_1.append(counter)\n",
    "df['badword'] = badword_0_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6559c533-c97c-4e58-879f-77c37417c083",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Creating two corpora of tweets as str\n",
    "- token0: tokenized and cleaned corpus of not hs\n",
    "- token0: tokenized and cleaned corpus of hs\n",
    "- sent: list of lists of tokenized tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7944bf28-1993-42b3-af60-254f56fef370",
   "metadata": {},
   "outputs": [],
   "source": [
    "token0, token1, sent = '', '', []\n",
    "custom_punctuation = string.punctuation + \"’‘\" + '’' + '' + '``' + \"''\" + '“' + '»' + '«' + '”' + '-'\n",
    "\n",
    "for tweet, label in zip(df['text_processed'], df['hs']):\n",
    "    tweet_ = tweet\n",
    "    tweet_ = word_tokenize(tweet_)\n",
    "    tweet_ = [word for word in tweet_ if word not in stopwords.words('italian') and word not in custom_punctuation]\n",
    "\n",
    "    if label == 0:\n",
    "        token0 += tweet\n",
    "    else:\n",
    "        token1 += tweet\n",
    "    sent.append(tweet_)\n",
    "\n",
    "\n",
    "token0 = word_tokenize(token0)\n",
    "token1 = word_tokenize(token1)\n",
    "token0 = [word for word in token0 if word not in stopwords.words('italian') and word not in custom_punctuation]\n",
    "token1 = [word for word in token1 if word not in stopwords.words('italian') and word not in custom_punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "d512ef15-a365-46fd-b026-ea12476a4d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rom', 1090), ('migranti', 727), ('immigrati', 399), ('italia', 336), ('roma', 300), ('campo', 289), ('italiani', 235), ('stranieri', 224), ('nomadi', 223), ('solo', 216)]\n",
      "\n",
      "\n",
      "[('immigrati', 515), ('migranti', 475), ('rom', 340), ('italiani', 324), ('italia', 299), ('casa', 226), ('stranieri', 207), ('clandestini', 202), ('solo', 189), ('terroristi', 184)]\n"
     ]
    }
   ],
   "source": [
    "freq_dist0 = nltk.FreqDist(token0)\n",
    "freq_dist1 = nltk.FreqDist(token1)\n",
    "\n",
    "print(freq_dist0.most_common(10))\n",
    "print('\\n')\n",
    "print(freq_dist1.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "b47080f6-2067-4066-ac58-27279a65dbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of list, where each sublist contains the tokenized version of the tweet\n",
    "# sents = [nltk.word_tokenize(text) for text in df['text_processed']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab76a655-0665-45d3-ac1f-24bfba20844d",
   "metadata": {},
   "source": [
    "## Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "173d27eb-28c8-4e4f-be87-d4685a0be617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111548, 6837)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number as expected decrease, as in the first attempt the pre-processing was less involving ad the result of this code was: (129277, 6837)\n",
    "len(Phrases(sents).vocab), len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "2a7f7200-8cd1-453f-8136-41cbc043e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phraser(Phrases(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "95400376-1198-47b0-a7d7-69f454b03f6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>collocation</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>radical_chic</td>\n",
       "      <td>4576.328205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>forze_dell'ordine</td>\n",
       "      <td>3541.206349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>asia_bibi</td>\n",
       "      <td>3346.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>global_compact</td>\n",
       "      <td>2845.612245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>&amp;_gt</td>\n",
       "      <td>2237.845679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>i_numeri</td>\n",
       "      <td>10.056164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>chi_arriva</td>\n",
       "      <td>10.043036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>basta_con</td>\n",
       "      <td>10.021832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>chi_ha</td>\n",
       "      <td>10.006714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>faccia_della</td>\n",
       "      <td>10.000717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>686 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           collocation        score\n",
       "677       radical_chic  4576.328205\n",
       "385  forze_dell'ordine  3541.206349\n",
       "454          asia_bibi  3346.440000\n",
       "684     global_compact  2845.612245\n",
       "624               &_gt  2237.845679\n",
       "..                 ...          ...\n",
       "309           i_numeri    10.056164\n",
       "242         chi_arriva    10.043036\n",
       "561          basta_con    10.021832\n",
       "373             chi_ha    10.006714\n",
       "512       faccia_della    10.000717\n",
       "\n",
       "[686 rows x 2 columns]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collocations = list()\n",
    "for key, score in bigram.phrasegrams.items():\n",
    "    collocations.append((key,score))\n",
    "bigrams_df = pd.DataFrame(collocations,columns = ['collocation', 'score'])\n",
    "bigrams_df.sort_values(by=['score'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "814879fe-120f-456a-8ca6-0f01e616865d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words('italian')\n",
    "sents_w_bigram = bigram[sents]\n",
    "trigrams = Phraser(Phrases(sents_w_bigram, connector_words=stopword_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "9c731fa9-786e-4129-ab90-e4b9c4f45c1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>collocation</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>corriere_:</td>\n",
       "      <td>11.530484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mafia_capitale</td>\n",
       "      <td>5219.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>campi_rom</td>\n",
       "      <td>262.157645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>!_!</td>\n",
       "      <td>29.309198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>studentessa_cinese</td>\n",
       "      <td>194874.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>sindaco_di_riace</td>\n",
       "      <td>549.457895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>anime_belle</td>\n",
       "      <td>14913.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>global_compact</td>\n",
       "      <td>195744.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>--_--</td>\n",
       "      <td>7733.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>--_--_--_--</td>\n",
       "      <td>6444.259259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            collocation          score\n",
       "0            corriere_:      11.530484\n",
       "1        mafia_capitale    5219.850000\n",
       "2             campi_rom     262.157645\n",
       "3                   !_!      29.309198\n",
       "4    studentessa_cinese  194874.400000\n",
       "..                  ...            ...\n",
       "193    sindaco_di_riace     549.457895\n",
       "194         anime_belle   14913.857143\n",
       "195      global_compact  195744.375000\n",
       "196               --_--    7733.111111\n",
       "197         --_--_--_--    6444.259259\n",
       "\n",
       "[198 rows x 2 columns]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collocations = list()\n",
    "for key, score in trigrams.phrasegrams.items():\n",
    "    collocations.append((key,score))\n",
    "trigrams_df = pd.DataFrame(collocations,columns = ['collocation', 'score'])\n",
    "trigrams_df.sort_values(by=['score'],ascending=False)\n",
    "trigrams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "54468c16-0e50-42d5-9e41-89f3078ce61b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>collocation</th>\n",
       "      <th>score</th>\n",
       "      <th>ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>studentessa_cinese_morta</td>\n",
       "      <td>50925.365854</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ultimenotizie_news_notizie</td>\n",
       "      <td>13384.230769</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>tenta_di_investire</td>\n",
       "      <td>9490.636364</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>stile_di_vita</td>\n",
       "      <td>7765.066116</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>--_--_--_--</td>\n",
       "      <td>6444.259259</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>&amp;_gt_;</td>\n",
       "      <td>3728.464286</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>reddito_di_cittadinanza</td>\n",
       "      <td>3222.129630</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>&amp;_amp_;</td>\n",
       "      <td>2237.078571</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>fermato_un_ventenne</td>\n",
       "      <td>2109.030303</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>corriere_della_sera</td>\n",
       "      <td>1999.942529</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>favoreggiamento_dell'immigrazione_clandestina</td>\n",
       "      <td>1581.772727</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>vitto_e_alloggio</td>\n",
       "      <td>1054.515152</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>giornata_della_memoria</td>\n",
       "      <td>994.257143</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-_il_messaggero</td>\n",
       "      <td>644.425926</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>milioni_di_euro</td>\n",
       "      <td>627.009009</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>sindaco_di_riace</td>\n",
       "      <td>549.457895</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>campo_rom_a_roma_nord</td>\n",
       "      <td>414.273810</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>dipendenti_della_lidl</td>\n",
       "      <td>392.469925</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>chiudere_i_campi</td>\n",
       "      <td>203.900391</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>-_il_fatto_quotidiano</td>\n",
       "      <td>154.662222</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>euro_al_giorno</td>\n",
       "      <td>145.941752</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>religione_di_pace</td>\n",
       "      <td>91.522794</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>nomade_del_campo</td>\n",
       "      <td>81.687793</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>fare_in_culo</td>\n",
       "      <td>80.833914</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>migliaia_di_migranti</td>\n",
       "      <td>78.482183</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>\\_''_:</td>\n",
       "      <td>69.182903</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>pena_di_morte</td>\n",
       "      <td>57.203836</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>donne_e_bambini</td>\n",
       "      <td>54.994030</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>dopo_un_furto</td>\n",
       "      <td>51.427094</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>morta_a_roma</td>\n",
       "      <td>49.311534</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>venire_in_italia</td>\n",
       "      <td>39.454649</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>prima_gli_italiani</td>\n",
       "      <td>34.173250</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>via_di_salone</td>\n",
       "      <td>29.112381</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>milioni_di_italiani</td>\n",
       "      <td>27.186719</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>crimini_degli_immigrati</td>\n",
       "      <td>25.966173</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>vengono_in_italia</td>\n",
       "      <td>24.369048</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>ancora_una_volta</td>\n",
       "      <td>21.996839</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>rom_e_sinti</td>\n",
       "      <td>19.870691</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>dato_di_fatto</td>\n",
       "      <td>18.749461</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>sicurezza_e_immigrazione</td>\n",
       "      <td>17.823298</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>milioni_di_immigrati</td>\n",
       "      <td>14.714165</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>business_dell_’_immigrazione</td>\n",
       "      <td>11.882199</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>islam_religione_di_pace</td>\n",
       "      <td>10.843062</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>campi_rom_,</td>\n",
       "      <td>10.824175</td>\n",
       "      <td>ngram</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       collocation         score  ngram\n",
       "5                         studentessa_cinese_morta  50925.365854  ngram\n",
       "22                      ultimenotizie_news_notizie  13384.230769  ngram\n",
       "126                             tenta_di_investire   9490.636364  ngram\n",
       "141                                  stile_di_vita   7765.066116  ngram\n",
       "197                                    --_--_--_--   6444.259259  ngram\n",
       "184                                         &_gt_;   3728.464286  ngram\n",
       "46                         reddito_di_cittadinanza   3222.129630  ngram\n",
       "156                                        &_amp_;   2237.078571  ngram\n",
       "65                             fermato_un_ventenne   2109.030303  ngram\n",
       "33                             corriere_della_sera   1999.942529  ngram\n",
       "192  favoreggiamento_dell'immigrazione_clandestina   1581.772727  ngram\n",
       "93                                vitto_e_alloggio   1054.515152  ngram\n",
       "60                          giornata_della_memoria    994.257143  ngram\n",
       "19                                 -_il_messaggero    644.425926  ngram\n",
       "89                                 milioni_di_euro    627.009009  ngram\n",
       "193                               sindaco_di_riace    549.457895  ngram\n",
       "85                           campo_rom_a_roma_nord    414.273810  ngram\n",
       "68                           dipendenti_della_lidl    392.469925  ngram\n",
       "55                                chiudere_i_campi    203.900391  ngram\n",
       "77                           -_il_fatto_quotidiano    154.662222  ngram\n",
       "103                                 euro_al_giorno    145.941752  ngram\n",
       "168                              religione_di_pace     91.522794  ngram\n",
       "66                                nomade_del_campo     81.687793  ngram\n",
       "166                                   fare_in_culo     80.833914  ngram\n",
       "169                           migliaia_di_migranti     78.482183  ngram\n",
       "163                                         \\_''_:     69.182903  ngram\n",
       "159                                  pena_di_morte     57.203836  ngram\n",
       "9                                  donne_e_bambini     54.994030  ngram\n",
       "30                                   dopo_un_furto     51.427094  ngram\n",
       "29                                    morta_a_roma     49.311534  ngram\n",
       "149                               venire_in_italia     39.454649  ngram\n",
       "90                              prima_gli_italiani     34.173250  ngram\n",
       "50                                   via_di_salone     29.112381  ngram\n",
       "135                            milioni_di_italiani     27.186719  ngram\n",
       "146                        crimini_degli_immigrati     25.966173  ngram\n",
       "101                              vengono_in_italia     24.369048  ngram\n",
       "79                                ancora_una_volta     21.996839  ngram\n",
       "58                                     rom_e_sinti     19.870691  ngram\n",
       "34                                   dato_di_fatto     18.749461  ngram\n",
       "63                        sicurezza_e_immigrazione     17.823298  ngram\n",
       "87                            milioni_di_immigrati     14.714165  ngram\n",
       "186                   business_dell_’_immigrazione     11.882199  ngram\n",
       "178                        islam_religione_di_pace     10.843062  ngram\n",
       "28                                     campi_rom_,     10.824175  ngram"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collocations_df = pd.concat([bigrams_df, trigrams_df])\n",
    "collocations_df = collocations_df.drop_duplicates(subset = [\"collocation\"], keep='last')\n",
    "collocations_df['ngram'] = collocations_df['collocation'].apply(lambda x: 'bigram' if x.count('_') == 1 else 'ngram')\n",
    "collocations_df[collocations_df['ngram'] == 'ngram'].sort_values('score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ab3344-a990-4d86-8143-cfd4c0b79dff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Working again on the DF\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "fbdd0692-822e-4c4d-97bc-3c9e20cce263",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 11:19:27 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68f579fe25747979bb75cbda0d91201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 11:19:27 INFO: Downloaded file to C:\\Users\\marco\\stanza_resources\\resources.json\n",
      "2024-11-16 11:19:27 INFO: Loading these models for language: it (Italian):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2024-11-16 11:19:27 INFO: Using device: cpu\n",
      "2024-11-16 11:19:27 INFO: Loading: tokenize\n",
      "C:\\Users\\marco\\anaconda4\\Lib\\site-packages\\stanza\\models\\tokenization\\trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-16 11:19:27 INFO: Loading: mwt\n",
      "C:\\Users\\marco\\anaconda4\\Lib\\site-packages\\stanza\\models\\mwt\\trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-16 11:19:27 INFO: Loading: pos\n",
      "C:\\Users\\marco\\anaconda4\\Lib\\site-packages\\stanza\\models\\pos\\trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "C:\\Users\\marco\\anaconda4\\Lib\\site-packages\\stanza\\models\\common\\pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "C:\\Users\\marco\\anaconda4\\Lib\\site-packages\\stanza\\models\\common\\char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-16 11:19:27 INFO: Loading: lemma\n",
      "C:\\Users\\marco\\anaconda4\\Lib\\site-packages\\stanza\\models\\lemma\\trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-16 11:19:27 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Stanza pipeline for Italian language\n",
    "nlp = stanza.Pipeline('it', processors='tokenize,mwt,pos,lemma')\n",
    "\n",
    "# Apply lemmatization with Stanza\n",
    "df['lemmatized_txt'] = [[word.lemma for word in nlp(' '.join(words_list)).sentences[0].words] for words_list in df['tokenized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c018574-d365-4af2-9e12-3b4cbe0e86ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = '../results'\n",
    "\n",
    "df.to_csv(os.path.join(results_dir, 'text_dataset_cleaned.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "701cb0b4-e190-47dd-9fbb-db1be7fadb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# The existing models each support negative, neutral, and positive, represented by 0, 1, 2 respectively.\n",
    "sentiment = []\n",
    "for element in df['text']:\n",
    "    element = nlp(element)\n",
    "    sentiment.append(element.sentiment)\n",
    "df['sentiment'] = sentiment\n",
    "\n",
    "# useless because it assign 0 to all the records, so it is not useful in classification terms\n",
    "df.drop('sentiment', axis = 1, inplace= True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "416ae02b-05e1-4e42-bb09-8ebc2bc847a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pos tagging\n",
    "pos_num = []\n",
    "pos_str = []\n",
    "for element in df['text']:\n",
    "    element = nlp(element)\n",
    "    pos_inner_num = []\n",
    "    pos_inner_str = []\n",
    "    for token in element:\n",
    "        pos_inner_num.append(token.pos)\n",
    "        pos_inner_str.append(token.pos_)\n",
    "    pos_num.append(pos_inner_num)\n",
    "    pos_str.append(pos_inner_str)\n",
    "#df['pos_num'] = pos_num\n",
    "#df['pos_str'] = pos_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "56cea3c5-4e6d-4f40-bbe9-6208612deb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "for l in pos_str:\n",
    "    dic = {}\n",
    "    for element in l:\n",
    "        dic[element] = dic.get(element, 0)+1\n",
    "    final.append(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "a3a40aa7-05e8-430d-94c5-24ffc430df92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.DataFrame(final)\n",
    "df_pos = df_pos.fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "05c7b4b5-5bb7-414e-b261-7157f4cf7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df,df_pos], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "694328da-a729-44c2-85dc-3a25e9cae23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I took the next pieces of code from the notebook 6.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59bda87-95e0-4a1f-89b4-e128f9aa744b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list of 1 & 2) tokenized words without common words\n",
    "sentences = []\n",
    "for l in df['lemmatized_txt']:\n",
    "    sentences.append(l)\n",
    "# remove words that appear only 1\n",
    "frequency = defaultdict(int)\n",
    "for sentence in sentences:\n",
    "    for token in sentence:\n",
    "        frequency[token] += 1\n",
    "texts = [[token for token in sentence if frequency[token] > 1]for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822d347f-9dbf-40e7-b0e7-739fd902a9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without rare word: 82368 \n",
      "With rare words: 89411\n"
     ]
    }
   ],
   "source": [
    "COUNTER_no_rare_words, COUNTER_with_rare_words= 0, 0\n",
    "for l in texts:\n",
    "    for word in l:\n",
    "        COUNTER_no_rare_words +=1\n",
    "\n",
    "for l in sentences:\n",
    "    for word in l:\n",
    "        COUNTER_with_rare_words +=1\n",
    "\n",
    "print('Without rare word:', COUNTER_no_rare_words,'\\n' 'With rare words:', COUNTER_with_rare_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "f756c2d5-0cf2-4f85-942e-0e0cacc9cef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "tfidf = gensim.models.TfidfModel(corpus, smartirs='ntc')\n",
    "word_doc_matrix_tfidf = gensim.matutils.corpus2dense(tfidf[corpus],num_terms = len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1aad96-99de-4d4c-bf03-69a9eb7f1cbf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Creating the dataset with token (as features) and score (are records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "15a6bc60-e852-4bca-a590-a64886d4309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(word_doc_matrix_tfidf, columns = [s for s in df['text']], index = [k for k in dictionary.token2id.keys()])\n",
    "df1 = df1.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "a182e203-256a-49d5-aadd-beaba14d664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_reset = df1.reset_index(drop=True)\n",
    "df = pd.concat([df, df1_reset], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06178ae-d7b0-4b80-9b73-6b65e25cac1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12577b7c-aa0b-44cb-802e-2a657d7ddc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC # Linear Support Vector Classifier\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "c1cf1ac0-d8d1-42c9-8ad7-47f48e3a6350",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfml = df.drop(['id', 'text', 'tokenized', 'text_processed','sentence_token', 'lemmatized_txt'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "68223448-d4c0-4b91-866e-f9d019967073",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfml.drop('hs', axis=1).values\n",
    "y = dfml['hs'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d681d35-983a-4d82-9b18-1589a8f18bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 3 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marco\\anaconda4\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "numeric_columns = ['n_token', 'n_sentence', 'badword']\n",
    "df_text = dfml.drop(columns = ['n_token', 'n_sentence', 'badword', 'hs'])\n",
    "text_columns = df_text.columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('text', Pipeline([\n",
    "            ('tfidf', TfidfTransformer()),  # Apply TF-IDF to text columns\n",
    "            ('sel', SelectKBest(chi2, k=5000))  # Feature selection after TF-IDF\n",
    "        ]), text_columns),  ('num', StandardScaler(), numeric_columns)])\n",
    "\n",
    "bin_pipeline = Pipeline([('preprocessor', preprocessor),\n",
    "                         ('learner', LogisticRegression(random_state=42, max_iter=500, verbose = 1, solver = 'saga',penalty = 'l1'))])\n",
    "\n",
    "X = dfml.drop(columns=['hs'])  # Adjust with your target column name\n",
    "y = dfml['hs']  # Adjust with your target column name\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.02, random_state=42, stratify=y)\n",
    "bin_pipeline.fit(X_train, y_train)\n",
    "bin_predictions = bin_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a316e00a-d462-46f8-979b-9689b3f5f1b4",
   "metadata": {},
   "source": [
    "##### Classification with StandardScaler worsens\n",
    "##### This is the result of a classification with\n",
    "\n",
    "* punctuation\n",
    "* bad_words count\n",
    "* n_of token and sentences\n",
    "* Count of pos --> without this the classification worsen (Macro AVG F1 0.68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "137a9104-d167-47e6-b26c-a4a64cb8db69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.73      0.74       204\n",
      "           1       0.62      0.66      0.64       138\n",
      "\n",
      "    accuracy                           0.70       342\n",
      "   macro avg       0.69      0.69      0.69       342\n",
      "weighted avg       0.70      0.70      0.70       342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred,zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "43f26c62-7d7c-4323-a7f2-80d068f08913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.91      0.83        82\n",
      "           1       0.82      0.56      0.67        55\n",
      "\n",
      "    accuracy                           0.77       137\n",
      "   macro avg       0.79      0.74      0.75       137\n",
      "weighted avg       0.78      0.77      0.76       137\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marco\\anaconda4\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, bin_predictions,zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd011e0-ea1d-41c4-94dd-a7d720cbcca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
