{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "hurtlex_file = \"revised_hurtlex.tsv\"\n",
    "\n",
    "lexicon_list = []\n",
    "with open(os.path.join(data_dir, hurtlex_file)) as infile:\n",
    "    reader = csv.DictReader(infile, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        lexicon_list.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pos', 'category', 'lemma', 'offensiveness_score'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon_list[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qas', 'ddf', '']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon_list[10]['category'].split(';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(item):\n",
    "    try:\n",
    "        score = float(item['offensiveness_score'])\n",
    "    except TypeError:\n",
    "        score = 0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories(item):\n",
    "    category_list = item['category'].split(';')\n",
    "    categories = [cat.strip() for cat in category_list if cat]\n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'an',\n",
       " 'asf',\n",
       " 'asm',\n",
       " 'cds',\n",
       " 'ddf',\n",
       " 'ddp',\n",
       " 'dfc',\n",
       " 'dm',\n",
       " 'dmc',\n",
       " 'is',\n",
       " 'mal',\n",
       " 'mi',\n",
       " 'min',\n",
       " 'om',\n",
       " 'op',\n",
       " 'or',\n",
       " 'pa',\n",
       " 'pr',\n",
       " 'ps',\n",
       " 'qas',\n",
       " 'rci',\n",
       " 're',\n",
       " 'svp'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([cat\n",
    "     for item in lexicon_list\n",
    "     for cat in get_categories(item)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_dict = {\n",
    "    item['lemma'] : (get_score(item), get_categories(item))\n",
    "    for item in lexicon_list\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.9338529209592425, ['qas'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon_dict.get(\"zavorra\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global offensiveness score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simply attribute to each document an offensiveness score which is the sum of the scores of all lemmas in the document.\n",
    "\n",
    "We use the text already lemmatized with the Stanza pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fab/.anaconda3/envs/txa_project/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "results_dir = \"../results\"\n",
    "pickle_file = \"stanza_proc_train.pkl\"\n",
    "\n",
    "with open(os.path.join(results_dir, pickle_file), 'rb') as infile:\n",
    "    train_set = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = train_set[100]['proc_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offensiveness_score(document):\n",
    "    score = 0\n",
    "    for word in document['proc_text'].iter_words():\n",
    "        score += lexicon_dict.get(word.lemma, (0, None))[0]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for document in train_set:\n",
    "    scores.append(get_offensiveness_score(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offensiveness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8482</th>\n",
       "      <td>30.823949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9180</th>\n",
       "      <td>25.760301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8705</th>\n",
       "      <td>25.418626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6946</th>\n",
       "      <td>23.854857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9201</th>\n",
       "      <td>23.581941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      offensiveness_score\n",
       "8482            30.823949\n",
       "9180            25.760301\n",
       "8705            25.418626\n",
       "6946            23.854857\n",
       "9201            23.581941"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "scores_df = pd.DataFrame(scores, index=[doc['id'] for doc in train_set], columns=['offensiveness_score'])\n",
    "scores_df.sort_values('offensiveness_score', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.to_csv(os.path.join(results_dir, 'offensiveness_train.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offensiveness by category\n",
    "\n",
    "Each offensive term in the hurtlex lexicon is related to one or more categories.\n",
    "We want to compute the offensiveness of each document with reference to each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offensiveness_score_by_cat(document):\n",
    "    scores = dict()\n",
    "    for word in document['proc_text'].iter_words():\n",
    "        score, categories = lexicon_dict.get(word.lemma, (0, None))\n",
    "        if score:\n",
    "            try:\n",
    "                for cat in categories:\n",
    "                    scores[cat] = scores.get(cat, 0) + score\n",
    "            except Exception:\n",
    "                print(categories)\n",
    "                break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for document in train_set:\n",
    "    scores.append(get_offensiveness_score_by_cat(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rci', 'ps', 'ddp', 'qas', 'dmc', 'cds', 're', 'svp', 'is', 'an', 'min',\n",
       "       'op', 'dfc', 'pr', 'asf', 'mi', 'asm', 'ddf', 'pa', 'mal', 'or', 'om'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_by_cat_df = pd.DataFrame(scores, index=[doc['id'] for doc in train_set]).fillna(0)\n",
    "scores_by_cat_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_by_cat_df.to_csv(os.path.join(results_dir, 'offensiveness_by_cat_train.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "txa_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
